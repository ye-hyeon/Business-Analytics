{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40b6c62a",
   "metadata": {},
   "source": [
    "# 텍스트마이닝\n",
    "\n",
    "\n",
    "## 텍스트 마이닝이란?\n",
    "* 텍스트로부터 높은 질의 정보를 뽑아내는 과정(패턴이나 트렌드를 파악함으로써)  *자연어처리, 통계학, 머신러닝, 딥러닝] 지식 필요* \n",
    "  + 일정한 길이의 벡터로 변환하면 머신러닝 기법 적용에 용이\n",
    "  + 리뷰 분석, 스팸 구분 등등 활용\n",
    " * NLP 기본도구 + 머신러닝\n",
    "\n",
    "## 분석방법\n",
    "* 단계: 문서(텍스트)-표준화(변형된 단어 등을 원형으로 변환)-단어로 이루어진 시퀀스(리스트)-순서가 없는 벡터, 문맥정보 포함하는 벡터, 워드 임베딩로 변형- 유형에 맞는 분석기법 사용\n",
    "\n",
    "### 도구\n",
    "* 파이썬: NLTK, Scikit, Gensim, Keras, PyTorch\n",
    "* NLP\n",
    "  + 목적: sparse vector(0의 값이 다수)로 변환\n",
    "  + 쪼개기-표준화-품사부착-말모둠으로 합치기/문서로 변환\n",
    " \n",
    "### 단계별\n",
    "* Tokenize: 문서를 문장으로 문장을 단어의 집합으로 분리, 의미없는 문장 분리, 영어는 공백을 기준으로 하기 때문에 한글보다 쉬움\n",
    "* Text Normalization: 동일한 의미의 단어가 다른 형태를 갖는 것을 보완. 의미가 아닌 규칙에 의한 변환인 어간추출/사전을 이용해 품사를 고려하는 표제어 추출 두 방법을 사용\n",
    "* POS-tagging: 형태소에 대해 품사를 할당하는 작업, 문맥에 따라 달라지는 동음어를 위해 문맥 파악이 필수\n",
    "* Chunking: 주어와 동사가 없는 두 단어 이상의 집합인 구를 찾기\n",
    "* 개체명 인식: 특정 정보를 포함한 명사구를 인식하여 관계를 추출\n",
    "* BOW: 의미있는 단어의 집합을 벡터로 변환하는 기법(시퀀스를 무시하고, 순서가 없는 집합으로 생각)\n",
    "  + vector space model\n",
    "  + count vector: 문제점(TFIDF: 단어의 count를 단어가 나타난 문서의 수로 나누어 자주 등장하지 않는 단어의 weight를 올림)\n",
    "\n",
    "### text classification\n",
    "* Naive Bayes, Logistic Regression(다중회귀분석), Ridge and Lasso Regression\n",
    "\n",
    "## 문제점 및 해결방법\n",
    "* 차원의 저주:의미있는 단어가 희소하여 각 데이터 간의 거리가 너무 멀게 위치하여 문제\n",
    "  + 차원을 줄여서 해결\n",
    "* Zipf's law: 극히 소수의 데이터가 결정적인 영향을 미치게 됨\n",
    "  + 빈도가 높은 단어 삭제, 1이상이면 1로 변환\n",
    "* 단어가 쓰인 순서정보의 손실: 단어의 순서를 중요시, 하지만 문맥이 중요함\n",
    "  + n-gram(부 분적 해결), 딥러닝\n",
    "  \n",
    "### Topic Modeling\n",
    "* 단어의 빈도는 주제를 유추가능함\n",
    "* Latnet Dirichlet Allocation\n",
    "\n",
    "### word embedding\n",
    "* one-hot-encoding으로 표현된 단어를 dense vector로 변환\n",
    "  + one-hot-encoding: 각 단어를 모든 문서에서 사용된 단어들의 수 길이의 벡터로 표현\n",
    "  + BOW는 단어에 대한 표현은 상관없이 통계량만 카운트하지만 위의 방법은 문맥을 고려\n",
    "* word2Vec: 단어의 위치에 기반하여 의미를 내포하는 벡터 생성, 단어의 유사성을 이용하여 연산이 가능 시맨틱\n",
    "* ELMo: 사전 훈련된 언어 모델을 사용하는 워드 임베딩 방법론, 이전 기법들이 동일한 단어가 문맥에 따라 전혀 다른 의미를 가지는 것을 반영하지 못하는 한계를 극복하기 위함\n",
    "* Document Embedding:word2Vec을 기반\n",
    "* RBM: 사전학습 목적으로 개발, 차원을 변경하면서 원래릐 정보량을 최대한 유지\n",
    "\n",
    "* sequence-to-sequence->attention->transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23836232",
   "metadata": {},
   "source": [
    "# 웹 크롤링1 - Static Crawling\n",
    "---\n",
    "# 1. urllib \n",
    "* 파이썬은 웹 사이트에 있는 데이터를 추출하기 위해 urllib 라이브러리 사용\n",
    "* 이를 이용해 HTTP 또는 FTP를 사용해 데이터 다운로드 가능\n",
    "* urllib은 URL을 다루는 모듈을 모아 놓은 패키지\n",
    "* urllib.request 모듈은 웹 사이트에 있는 데이터에 접근하는 기능 제공, 또한 인증, 리다렉트, 쿠키처럼 인터넷을 이용한 다양한 요청과 처리가 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "040258e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde6eadf",
   "metadata": {},
   "source": [
    "## 1.1 urllib.request를 이용한 다운로드\n",
    "* urllib.request 모듈에 있는 urlretrieve() 함수 이용\n",
    "* 다음의 코드는 PNG 파일을 test.png 라는 이름의 파일로 저장하는 예제임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1776096b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장되었습니다\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 읽어들이기 \n",
    "from urllib import request\n",
    "\n",
    "url=\"http://uta.pw/shodou/img/28/214.png\" # 웹 페이지 저장\n",
    "savename=\"test.png\"\n",
    "\n",
    "request.urlretrieve(url, savename) #  url에 해당하는 이미지를 다운받아 원하는 이름으로 저장\n",
    "print(\"저장되었습니다\") # \"\" 안 문자 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c0d7b1",
   "metadata": {},
   "source": [
    "## 1.2. urlopen으로 파일에 저장하는 방법\n",
    "* request.urlopen()은 메모리에 데이터를 올린 후 파일에 저장하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20da8798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장되었습니다..\n"
     ]
    }
   ],
   "source": [
    "# URL과 저장경로 지정하기\n",
    "url = \"http://uta.pw/shodou/img/28/214.png\"\n",
    "savename = \"test1.png\"\n",
    "#다운로드\n",
    "mem = request.urlopen(url).read()\n",
    "#파일로 저장하기, wb는 쓰기와 바이너리모드\n",
    "with open(savename, mode=\"wb\") as f:\n",
    "    f.write(mem)\n",
    "    print(\"저장되었습니다..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322826c5",
   "metadata": {},
   "source": [
    "## 1.3. API 사용하기\n",
    "### 클라이언트 접속 정보 출력 (기본)\n",
    "* API는 사용자의 요청에 따라 정보를 반환하는 프로그램\n",
    "* IP 주소, UserAgent 등 클라이언트 접속정보 출력하는 \"IP 확인 API\" 접근해서 정보를 추출하는 프로그램"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d08c8f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ip]\n",
      "API_URI=http://api.aoikujira.com/ip/get.php\n",
      "REMOTE_ADDR=61.253.164.75\n",
      "REMOTE_HOST=61.253.164.75\n",
      "REMOTE_PORT=48812\n",
      "HTTP_HOST=api.aoikujira.com\n",
      "HTTP_USER_AGENT=Python-urllib/3.8\n",
      "HTTP_ACCEPT_LANGUAGE=\n",
      "HTTP_ACCEPT_CHARSET=\n",
      "SERVER_PORT=80\n",
      "FORMAT=ini\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#데이터 읽어들이기\n",
    "url=\"http://api.aoikujira.com/ip/ini\" \n",
    "res=request.urlopen(url) #url 열기\n",
    "data=res.read() # 읽어오기\n",
    "\n",
    "#바이너리를 문자열로 변환하기\n",
    "text=data.decode(\"utf-8\")\n",
    "print(text) # 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd05d512",
   "metadata": {},
   "source": [
    "## 2. BeautifulSoup\n",
    "* 스크레이핑(Scraping or Crawling)이란 웹 사이트에서 데이터를 추출하고, 원하는 정보를 추출하는 것을 의미\n",
    "* BeautifulSoup란 파이썬으로 스크레이핑할 때 사용되는 라이브러리로서 HTML/XML에서 정보를 추출할 수 있도록 도와줌. 그러나 다운로드 기능은 없음.\n",
    "* 파이썬 라이브러리는 pip 명령어를 이용해 설치 가능. Python Package Index(PyPI)에 있는 패키지 명령어를 한줄로 설치 가능\n",
    "  + URL (http://pypi.python.org/pypi)\n",
    "pip install beautifulsoup4\n",
    "\n",
    "* 예제 HTML\n",
    "\n",
    " <html><body>\n",
    "   <h1>스크레이핑이란?</h1>\n",
    "   <p>웹 페이지를 분석하는 것</p>>   <p>원하는 부분을 추출하는 것</p>\n",
    " </body></html>\n",
    "\n",
    "### 패키지 import 및 예제 HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0f0ad4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = \"\"\"\n",
    "<html><body>\n",
    "  <h1>스크레이핑이란?</h1>\n",
    "  <p>웹 페이지를 분석하는 것</p>\n",
    "  <p>원하는 부분을 추출하는 것</p>\n",
    "</body></html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc46e7b0",
   "metadata": {},
   "source": [
    "## 2.1. 기본 사용¶\n",
    "* 다음은 Beautifulsoup를 이용하여 웹사이트로부터 HTML을 가져와 문자열로 만들어 이용하는 예제임\n",
    "* h1 태그를 접근하기 위해 html-body-h1 구조를 사용하여 soup.html.body.h1 이런식으로 이용하게 됨.\n",
    "* p 태그는 두개가 있어 soup.html.body.p 한 후 next_sibling을 두번 이용하여 다음 p를 추출. 한번만 하면 그 다음 공백이 추출됨.\n",
    "* HTML 태그가 복잡한 경우 이런 방식으로 계속 진행하기는 적합하지 않음.\n",
    "\n",
    "### 2) HTML 분석하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cf2d2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'html.parser') # 사이트로부터 html을 받아 문자열로 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a814df",
   "metadata": {},
   "source": [
    "### 3) 원하는 부분 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e4af2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = soup.html.body.h1 # h1태그에 접근\n",
    "p1 = soup.html.body.p # p태그에 접근\n",
    "p2 = p1.next_sibling.next_sibling # 두번째의 p태그에 접근"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15b1125",
   "metadata": {},
   "source": [
    "### 4) 요소의 글자 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27f1de24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1 = 스크레이핑이란?\n",
      "p  = 웹 페이지를 분석하는 것\n",
      "p  = 원하는 부분을 추출하는 것\n"
     ]
    }
   ],
   "source": [
    "print(f\"h1 = {h1.string}\") # h1 태그의 문자열 출력\n",
    "print(f\"p  = {p1.string}\") # p1 태그의 문자열 출력\n",
    "print(f\"p  = {p2.string}\") # p2 태그의 문자열 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dee2f9d",
   "metadata": {},
   "source": [
    "## 2.2 요소를 찾는 method\n",
    "### 단일 element 추출: find()\n",
    "BeautifulSoup는 루트부터 하나하나 요소를 찾는 방법 말고도 find()라는 메소드를 제공함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "606033c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'html.parser')# 파싱한 결과를 BeautifulSoup로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956657b4",
   "metadata": {},
   "source": [
    "* 1) find() 메서드로 원하는 부분 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1d75258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>스크레이핑이란?</h1>\n"
     ]
    }
   ],
   "source": [
    "title = soup.find(\"h1\") # h1 태그 추출 후 저장\n",
    "body  = soup.find(\"p\") # P 캐그 추출 후 저장\n",
    "print(title) # 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ac23ca",
   "metadata": {},
   "source": [
    "* 2) 텍스트 부분 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "172f0755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#title = 스크레이핑이란?\n",
      "#body = 웹 페이지를 분석하는 것\n"
     ]
    }
   ],
   "source": [
    "print(f\"#title = {title.string}\" )\n",
    "print(f\"#body = {body.string}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcf1056",
   "metadata": {},
   "source": [
    "### 복수 elements 추출: find_all()\n",
    "여러개의 태그를 한번에 추출하고자 할때 사용함. 다음의 예제에서는 여러개의 태그를 추출하는 법을 보여주고 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92dfe9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = \"\"\"\n",
    "<html><body>\n",
    "  <ul>\n",
    "    <li><a href=\"http://www.naver.com\">naver</a></li>\n",
    "    <li><a href=\"http://www.daum.net\">daum</a></li>\n",
    "  </ul>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser') # 파싱한 결과를 BeautifulSoup로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21396d8e",
   "metadata": {},
   "source": [
    "* 1) find_all() 메서드로 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a05f593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a href=\"http://www.naver.com\">naver</a>, <a href=\"http://www.daum.net\">daum</a>] 2\n"
     ]
    }
   ],
   "source": [
    "links = soup.find_all(\"a\") # 주어진 조건에 맞는 태그를 리스트로 찾아내는\n",
    "print(links, len(links)) # 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2662cff",
   "metadata": {},
   "source": [
    "* 2) 링크 목록 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69896b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naver > http://www.naver.com\n",
      "daum > http://www.daum.net\n"
     ]
    }
   ],
   "source": [
    "for a in links:\n",
    "    href = a.attrs['href'] # href의 속성에 있는 속성값을 추출\n",
    "    text = a.string \n",
    "    print(text, \">\", href) # 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d966b7",
   "metadata": {},
   "source": [
    "# 3. Css Selector¶\n",
    "> Css Selector란, 웹상의 요소에 css를 적용하기 위한 문법으로, 즉 요소를 선택하기 위한 패턴입니다.\n",
    "\n",
    "> 출처: https://www.w3schools.com/cssref/css_selectors.asp\n",
    "\n",
    "앞서 간단하게 태그를 사용하여 데이터를 추출하는 방법에 대해서 살펴보았습니다.\n",
    "\n",
    "하지만 복잡하게 구조화된 웹 사이트에서 자신이 원하는 데이터를 가져오기 위해서는 Css Selector에 대한 이해가 필요합니다.\n",
    "\n",
    "|**서식**|**설명**|\n",
    "|--|--|\n",
    "|*|모든요소를 선택|\n",
    "|<요소이름>|요소 이름을 기반으로 선택|\n",
    "|클래스 이름|클래스 이름을 기반으로 선택|\n",
    "|#<id이름>|id 속성을 기반으로 선택|\n",
    "\n",
    "## BeautifulSoup에서 Css Selector 사용하기¶\n",
    "BeautifulSoup에서는 Css Selector로 값을 가져올 수 있도록 find와는 다른 다음과 같은 메서드를 제공합니다.\n",
    "\n",
    "|**메서드**|**설명**|\n",
    "|--|--|\n",
    "|soup.select_one(선택자)|CSS 선택자로 요소 하나를 추출합니다|\n",
    "|soup.select(선택자)|CSS 선택자오 요소 여러 개를 리스트를 추출합니다|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c3d2941",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = \"\"\"\n",
    "<html><body>\n",
    "<div id=\"meigen\">\n",
    "  <h1>위키북스 도서</h1>\n",
    "  <ul class=\"items\">\n",
    "    <li>유니티 게임 이펙트 입문</li>\n",
    "    <li>스위프트로 시작하는 아이폰 앱 개발 교과서</li>\n",
    "    <li>모던 웹사이트 디자인의 정석</li>\n",
    "  </ul>\n",
    "</div>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "\n",
    "# HTML 분석하기 \n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3d5771",
   "metadata": {},
   "source": [
    "* 필요한 부분을 CSS 쿼리로 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99aa5ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1 = 위키북스 도서\n",
      "li = 유니티 게임 이펙트 입문\n",
      "li = 스위프트로 시작하는 아이폰 앱 개발 교과서\n",
      "li = 모던 웹사이트 디자인의 정석\n"
     ]
    }
   ],
   "source": [
    "# 타이틀 부분 추출하기 --- (※3)\n",
    "h1 = soup.select_one(\"div#meigen > h1\").string # h1 태그 불러오기\n",
    "print(f\"h1 = {h1}\")\n",
    "\n",
    "# 목록 부분 추출하기 --- (※4)\n",
    "li_list = soup.select(\"div#meigen > ul.items > li\")\n",
    "for li in li_list:\n",
    "  print(f\"li = {li.string}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab20e08",
   "metadata": {},
   "source": [
    "# 4. 활용 예제\n",
    "앞서 배운 urllib과 BeautifulSoup를 조합하면, 웹스크레이핑 및 API 요청 작업을 쉽게 수행하실 수 있습니다.\n",
    "\n",
    "1. URL을 이용하여 웹으로부터 html을 읽어들임 (urllib)\n",
    "2. html 분석 및 원하는 데이터를 추출 (BeautifulSoup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5003406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # 라이브러리 호출\n",
    "from urllib import request, parse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0567e27",
   "metadata": {},
   "source": [
    "## 4.1. 네이버 금융 - 환율 정보\n",
    "* 다양한 금융 정보가 공개돼 있는 \"네이버 금융\"에서 원/달러 환율 정보를 추출해보자!\n",
    "* 네이버 금융의 시장 지표 페이지 https://finance.naver.com/marketindex/\n",
    "* 다음은 원/달러 환율 정보를 추출하는 프로그램임\n",
    "\n",
    "### 1) HTML 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6f813a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://finance.naver.com/marketindex/\"\n",
    "res = request.urlopen(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c07aab5",
   "metadata": {},
   "source": [
    "### 2) HTML 분석하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "369326cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8f99fe",
   "metadata": {},
   "source": [
    "### 3) 원하는 데이터 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c069a773",
   "metadata": {},
   "outputs": [],
   "source": [
    "price = soup select_one(\"div head_info > span value\") string \n",
    "print(\"usd.krw=\", price) # 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b7383a",
   "metadata": {},
   "source": [
    "## 4.2. 기상청 RSS\n",
    "* 기상청 RSS에서 특정 내용을 추출하는 예제\n",
    "* 기상청 RSS에서 XML 데이터를 추출하고 XML 내용을 출력\n",
    "* 기상청의 RSS 서비스에 지역 번호를 지정하여 데이터 요청해보기 http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp\n",
    "  + 참고: 기상청 RSS http://www.kma.go.kr/weather/lifenindustry/service_rss.jsp\n",
    "  \n",
    "\n",
    "|**매개변수**|**의미**|\n",
    "|--|--|\n",
    "|shnid|기상정보를 알고 싶은 지역을 지정|\n",
    "* 지역번호는 다음과 같음\n",
    "\n",
    "|**지역**-|**지역번호**|**지역**|**지역번호**|\n",
    "|--|--|\n",
    "|전국|108|전라북도|146|\n",
    "|서울/경기도|109|전라남도|156|\n",
    "|강원도|105|경상북도|143|\n",
    "|충청북도|131|경상남도|159\n",
    "|충청남도|133|제주특별자치도|184|\n",
    "* 파이썬으로 요청 전용 매개변수를 만들 때는 urllib.parse 모듈의 urlencode() 함수를 사용해 매개변수를 URL로 인코딩한다.\n",
    "### 1) HTML 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1394bc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url= http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp?stnId=109\n"
     ]
    }
   ],
   "source": [
    "url = \"http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp\"\n",
    "\n",
    "#매개변수를 URL로 인코딩한다.\n",
    "values = {\n",
    "    'stnId':'109'\n",
    "}\n",
    "\n",
    "params=parse.urlencode(values) # 매개변수를 url로 인코딩하여 저장\n",
    "url += \"?\"+params # URL에 매개변수 추가\n",
    "print(\"url=\", url)\n",
    "\n",
    "res = request.urlopen(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1582c83a",
   "metadata": {},
   "source": [
    "### 2) HTML 분석하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e39cfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420f8003",
   "metadata": {},
   "source": [
    "### 3) 원하는 데이터 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f84daf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "서울,경기도 육상중기예보\n",
      "○ (강수) 29일(수) 오후에는 비가 내리겠습니다.<br />○ (기온) 이번 예보기간 아침최저기온은 14~19도, 낮최고기온은 24~27도로 오늘(22일, 아침최저기온 16~18도, 낮최고기온 23~26도)과 비슷하겠습니다.<br />○ (해상) 서해중부해상의 물결은 1.0~2.0m로 일겠습니다.<br />○ (주말전망) 25일(토)과 26일(일)은 구름많겠습니다. 아침최저기온은 14~19도, 낮최고기온은 25~27도가 되겠습니다.\n"
     ]
    }
   ],
   "source": [
    "header = soup.find(\"header\")\n",
    "\n",
    "title = header.find(\"title\").text # title 의 텍스트 저장\n",
    "wf = header.find(\"wf\").text # wf의 텍스트 저장\n",
    "\n",
    "print(title) # 출력\n",
    "print(wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6f7a30",
   "metadata": {},
   "source": [
    "* css selector 기반"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1cdb9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "서울,경기도 육상중기예보\n",
      "○ (강수) 29일(수) 오후에는 비가 내리겠습니다.<br />○ (기온) 이번 예보기간 아침최저기온은 14~19도, 낮최고기온은 24~27도로 오늘(22일, 아침최저기온 16~18도, 낮최고기온 23~26도)과 비슷하겠습니다.<br />○ (해상) 서해중부해상의 물결은 1.0~2.0m로 일겠습니다.<br />○ (주말전망) 25일(토)과 26일(일)은 구름많겠습니다. 아침최저기온은 14~19도, 낮최고기온은 25~27도가 되겠습니다.\n"
     ]
    }
   ],
   "source": [
    "title = soup.select_one(\"header > title\").text # title의 텍스트 추출\n",
    "wf = header.select_one(\"header wf\").text # wf의 텍스트 추출\n",
    "\n",
    "print(title) # 출력\n",
    "print(wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca0dad1",
   "metadata": {},
   "source": [
    "## 4.3. 윤동주 작가의 작품 목록\n",
    "* 위키문헌 (https://ko.wikisource.org/wiki) 에 공개되어 있는 윤동주의 작품목록을 가져오기\n",
    "* 윤동주 위키 (https://ko.wikisource.org/wiki/%EC%A0%80%EC%9E%90:%EC%9C%A4%EB%8F%99%EC%A3%BC)\n",
    "* 하늘과 바람과 시 부분을 선택한 후 오른쪽 마우스 이용해 copy selector로 카피하면 다음의 CSS 선택자가 카피됨\n",
    "  + #mw-content-text > div > ul:nth-child(6) > li > b > a\n",
    "* nth-child(n) 은 n 번째 요소를 의미 즉 6번째 요소를 의미, #mw-content-text 내부에 있는 url 태그는 모두 작품과 관련된 태그. 따라서 따로 구분할 필요는 없으며 생략해도 됨. BeautifulSoup는 nth-child 지원하지 않음\n",
    "  + Recall PR7 Problem1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9198f2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 하늘과 바람과 별과 시\n",
      "- 증보판\n",
      "- 서시\n",
      "- 자화상\n",
      "- 소년\n",
      "- 눈 오는 지도\n",
      "- 돌아와 보는 밤\n",
      "- 병원\n",
      "- 새로운 길\n",
      "- 간판 없는 거리\n",
      "- 태초의 아침\n",
      "- 또 태초의 아침\n",
      "- 새벽이 올 때까지\n",
      "- 무서운 시간\n",
      "- 십자가\n",
      "- 바람이 불어\n",
      "- 슬픈 족속\n",
      "- 눈감고 간다\n",
      "- 또 다른 고향\n",
      "- 길\n",
      "- 별 헤는 밤\n",
      "- 흰 그림자\n",
      "- 사랑스런 추억\n",
      "- 흐르는 거리\n",
      "- 쉽게 씌어진 시\n",
      "- 봄\n",
      "- 참회록\n",
      "- 간(肝)\n",
      "- 위로\n",
      "- 팔복\n",
      "- 못자는밤\n",
      "- 달같이\n",
      "- 고추밭\n",
      "- 아우의 인상화\n",
      "- 사랑의 전당\n",
      "- 이적\n",
      "- 비오는 밤\n",
      "- 산골물\n",
      "- 유언\n",
      "- 창\n",
      "- 바다\n",
      "- 비로봉\n",
      "- 산협의 오후\n",
      "- 명상\n",
      "- 소낙비\n",
      "- 한난계\n",
      "- 풍경\n",
      "- 달밤\n",
      "- 장\n",
      "- 밤\n",
      "- 황혼이 바다가 되어\n",
      "- 아침\n",
      "- 빨래\n",
      "- 꿈은 깨어지고\n",
      "- 산림\n",
      "- 이런날\n",
      "- 산상\n",
      "- 양지쪽\n",
      "- 닭\n",
      "- 가슴 1\n",
      "- 가슴 2\n",
      "- 비둘기\n",
      "- 황혼\n",
      "- 남쪽 하늘\n",
      "- 창공\n",
      "- 거리에서\n",
      "- 삶과 죽음\n",
      "- 초한대\n",
      "- 산울림\n",
      "- 해바라기 얼굴\n",
      "- 귀뚜라미와 나와\n",
      "- 애기의 새벽\n",
      "- 햇빛·바람\n",
      "- 반디불\n",
      "- 둘 다\n",
      "- 거짓부리\n",
      "- 눈\n",
      "- 참새\n",
      "- 버선본\n",
      "- 편지\n",
      "- 봄\n",
      "- 무얼 먹구 사나\n",
      "- 굴뚝\n",
      "- 햇비\n",
      "- 빗자루\n",
      "- 기왓장 내외\n",
      "- 오줌싸개 지도\n",
      "- 병아리\n",
      "- 조개껍질\n",
      "- 겨울\n",
      "- 트루게네프의 언덕\n",
      "- 달을 쏘다\n",
      "- 별똥 떨어진 데\n",
      "- 화원에 꽃이 핀다\n",
      "- 종시\n"
     ]
    }
   ],
   "source": [
    "# 뒤의 인코딩 부분은 \"저자:윤동주\"라는 의미입니다.\n",
    "# 따로 입력하지 말고 위키 문헌 홈페이지에 들어간 뒤에 주소를 복사해서 사용하세요.\n",
    "\n",
    "url = \"https://ko.wikisource.org/wiki/%EC%A0%80%EC%9E%90:%EC%9C%A4%EB%8F%99%EC%A3%BC\"\n",
    "res = request.urlopen(url)\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "# #mw-content-text 바로 아래에 있는 \n",
    "# ul 태그 바로 아래에 있는\n",
    "# li 태그 아래에 있는\n",
    "# a 태그를 모두 선택합니다.\n",
    "a_list = soup.select(\"#mw-content-text   ul > li  a\")\n",
    "for a in a_list:\n",
    "    name = a.string\n",
    "    print(f\"- {name}\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e487a766",
   "metadata": {},
   "source": [
    "# 일반문제\n",
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56328efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib import request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640314dc",
   "metadata": {},
   "source": [
    "## 1. 네이버 뉴스 헤드라인\n",
    "배운 내용을 바탕으로 네이버 뉴스(https://news.naver.com/)에서 헤드라인 뉴스의 제목을 추출해보고자 합니다.\n",
    "\n",
    "> Q: 다음의 코드에 css selector를 추가하여 최신 기사의 헤드라인을 스크레이핑하는 코드를 완성하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0512ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                                        '파산설' 중국 헝다 \"23일 위안화 채권 이자 지급\"\n",
      "                                    \n",
      "\n",
      "                                        美 싸이티바, 한국에 621억 투자…백신 원부자재 공장 짓는다\n",
      "                                    \n",
      "\n",
      "                                        전기요금 오를 듯… 연료비 상승에 한전 적자까지\n",
      "                                    \n",
      "\n",
      "                                        베트남에 백신 100만회분 공여…정부 \"우리 충분히 쓰고 주겠다\"\n",
      "                                    \n",
      "\n",
      "                                        김기현 \"이재명, 배임 혐의 고발…與, 특검·국조 동의해야\"\n",
      "                                    \n"
     ]
    }
   ],
   "source": [
    "url = \"https://news.naver.com/\" # urㅣ로 기사 불러오기\n",
    "\n",
    "res = request.urlopen(url) # url 불러오기\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "# 구분 분석 트리 생성 함수 사용\n",
    "selector = \"#today_main_news > div.hdline_news > ul > li > div.hdline_article_tit > a\"\n",
    "# 헤드라인 태그 저장\n",
    "for a in soup.select(selector):\n",
    "    title = a.text # 텍스트 저장\n",
    "    print(title) # 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9abcc0",
   "metadata": {},
   "source": [
    "## 2. 시민의 소리 게시판\n",
    "다음은 서울시 대공원의 시민의 소리 게시판 입니다.\n",
    "\n",
    "https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgList.do?pgno=1\n",
    "\n",
    "해당 페이지에 나타난 게시글들의 제목을 수집하고자 합니다.\n",
    "\n",
    "> Q: 다음의 코드에 css selector를 추가하여 해당 페이지에서 게시글의 제목을 스크레이핑하는 코드를 완성하시오. 또한 과제 제출시 하단의 추가 내용을 참고하여 수집한 데이터를 csv 형태로 저장하여 해당 csv 파일도 함께 제출하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cad3af31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['강창수 해설사님 ', '동물해설사님 칭찬', '강창수 동물 해설사님', '놀이동산 푸드코트 김치가 중국산인 이유는?', '주슨트 설명 최고예요!!', '강창수 주슨트님 최고 !!', 'ZOOCENT 스케줄표?', '호주동물 호주설명 ', '호주및 호주동물 설명에 대해 ', '동물원'] ['https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=l2RBC0TijEbyuSpP5ZN52qoT7gnHctb3RrEglFvbp3EJVKHepZ5UxakTwptVDVaR.etisw2_servlet_user?qnaid=QNAS20210920000001&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=l2RBC0TijEbyuSpP5ZN52qoT7gnHctb3RrEglFvbp3EJVKHepZ5UxakTwptVDVaR.etisw2_servlet_user?qnaid=QNAS20210919000004&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=l2RBC0TijEbyuSpP5ZN52qoT7gnHctb3RrEglFvbp3EJVKHepZ5UxakTwptVDVaR.etisw2_servlet_user?qnaid=QNAS20210919000003&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=l2RBC0TijEbyuSpP5ZN52qoT7gnHctb3RrEglFvbp3EJVKHepZ5UxakTwptVDVaR.etisw2_servlet_user?qnaid=QNAS20210918000002&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=l2RBC0TijEbyuSpP5ZN52qoT7gnHctb3RrEglFvbp3EJVKHepZ5UxakTwptVDVaR.etisw2_servlet_user?qnaid=QNAS20210909000001&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=l2RBC0TijEbyuSpP5ZN52qoT7gnHctb3RrEglFvbp3EJVKHepZ5UxakTwptVDVaR.etisw2_servlet_user?qnaid=QNAS20210908000004&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=l2RBC0TijEbyuSpP5ZN52qoT7gnHctb3RrEglFvbp3EJVKHepZ5UxakTwptVDVaR.etisw2_servlet_user?qnaid=QNAS20210906000002&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=l2RBC0TijEbyuSpP5ZN52qoT7gnHctb3RrEglFvbp3EJVKHepZ5UxakTwptVDVaR.etisw2_servlet_user?qnaid=QNAS20210904000006&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=l2RBC0TijEbyuSpP5ZN52qoT7gnHctb3RrEglFvbp3EJVKHepZ5UxakTwptVDVaR.etisw2_servlet_user?qnaid=QNAS20210904000004&pgno=1', 'https://www.sisul.or.kr/open_content/childrenpark/qna/qnaMsgDetail.do;jsessionid=l2RBC0TijEbyuSpP5ZN52qoT7gnHctb3RrEglFvbp3EJVKHepZ5UxakTwptVDVaR.etisw2_servlet_user?qnaid=QNAS20210904000003&pgno=1']\n"
     ]
    }
   ],
   "source": [
    "url_head = \"https://www.sisul.or.kr\"\n",
    "\n",
    "url_board = url_head + \"/open_content/childrenpark/qna/qnaMsgList.do?pgno=1\" # url 저장\n",
    "\n",
    "\n",
    "\n",
    "res = request.urlopen(url_board) # 웹 페이지 불러오기\n",
    "soup = BeautifulSoup(res, \"html.parser\") \n",
    "\n",
    "# selector = \"#detail_con > div.generalboard > table > tbody > tr > td.left.title > a\"\n",
    "selector = \"#detail_con > div.generalboard > table > tbody > tr > td.left.title > a\"\n",
    "titles = []\n",
    "links = []\n",
    "for a in soup.select(selector):\n",
    "    titles.append(a.text)\n",
    "    links.append(url_head + a.attrs[\"href\"])\n",
    "    \n",
    "print(titles, links)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f482e8a7",
   "metadata": {},
   "source": [
    "### 추가 내용\n",
    "수집된 자료를 데이터프레임으로 만들어 csv로 저장하는 것이 일반적입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a749a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>강창수 해설사님</td>\n",
       "      <td>https://www.sisul.or.kr/open_content/childrenp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>동물해설사님 칭찬</td>\n",
       "      <td>https://www.sisul.or.kr/open_content/childrenp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>강창수 동물 해설사님</td>\n",
       "      <td>https://www.sisul.or.kr/open_content/childrenp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>놀이동산 푸드코트 김치가 중국산인 이유는?</td>\n",
       "      <td>https://www.sisul.or.kr/open_content/childrenp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>주슨트 설명 최고예요!!</td>\n",
       "      <td>https://www.sisul.or.kr/open_content/childrenp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     title                                               link\n",
       "0                강창수 해설사님   https://www.sisul.or.kr/open_content/childrenp...\n",
       "1                동물해설사님 칭찬  https://www.sisul.or.kr/open_content/childrenp...\n",
       "2              강창수 동물 해설사님  https://www.sisul.or.kr/open_content/childrenp...\n",
       "3  놀이동산 푸드코트 김치가 중국산인 이유는?  https://www.sisul.or.kr/open_content/childrenp...\n",
       "4            주슨트 설명 최고예요!!  https://www.sisul.or.kr/open_content/childrenp..."
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "board_df = pd.DataFrame({\"title\": titles, \"link\": links})\n",
    "board_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac97fd61",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'board_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2b41a1cffa4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mboard_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"board.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'board_df' is not defined"
     ]
    }
   ],
   "source": [
    "board_df.to_csv(\"board.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18656da8",
   "metadata": {},
   "source": [
    "# (Optional) 웹 크롤링2 - Dynamic Crawling\n",
    "# 0. 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a24eec35",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-c8dd636d5d51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mby\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupport\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mui\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWebDriverWait\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupport\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexpected_conditions\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mEC\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ff175a",
   "metadata": {},
   "source": [
    "# 1. Selenium 기초\n",
    "자신의 크롬 버전을 확인하고 크롬 웹드라이버를 다운받아놓아야합니다.\n",
    "\n",
    "* 2020.09.13 기준 최신 버전: 85.0.4183.102\n",
    "\n",
    "\n",
    "\n",
    "## 1.1. Simple Text Crawling\n",
    "멜론 사이트에서 노래 제목을 크롤링해보자\n",
    "\n",
    "URL: https://www.melon.com/chart/index.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c2968d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'webdriver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-3a33ef917653>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m  \u001b[1;31m# chrome driver 설정\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdriver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDRIVER_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimplicitly_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'webdriver' is not defined"
     ]
    }
   ],
   "source": [
    "DRIVER_PATH = 'D:/dev_files/webdrivers/chrome/chromedriver85.exe'\n",
    "\n",
    " # chrome driver 설정\n",
    "driver = webdriver.Chrome(DRIVER_PATH) \n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "url = \"https://www.melon.com/chart/index.htm\"\n",
    "\n",
    "driver.get(url)\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# title crawling\n",
    "title = WebDriverWait(driver, 20) \\\n",
    "    .until(EC.presence_of_element_located((By.CSS_SELECTOR, \"#frm > div > table > tbody > tr:nth-child(1) > td:nth-child(4) > div > div\")))\n",
    "\n",
    "# print(\"Title: {}\".format(title.text))\n",
    "\n",
    "title.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24a7654",
   "metadata": {},
   "source": [
    "css selector의 규칙을 찾아본다\n",
    "\n",
    "* 1번째 제목: #frm > div > table > tbody > tr:nth-child(1) > td:nth-child(4) > div > div\"\n",
    "* 2번째 제목: #frm > div > table > tbody > tr:nth-child(2) > td:nth-child(4) > div > div\n",
    ". . .\n",
    "\n",
    "* 100번째 제목: #frm > div > table > tbody > tr:nth-child(100) > td:nth-child(4) > div > div\n",
    "또는 XPATH로도 확인해보자 (full Xpath)\n",
    "\n",
    "* 1번째 제목: //*[@id=\"frm\"]/div/table/tbody/tr[1]/td[4]/div/div\n",
    "* 2번째 제목: //*[@id=\"frm\"]/div/table/tbody/tr[2]/td[4]/div/div\n",
    ". . .\n",
    "\n",
    "* 50번째 제목: //*[@id=\"frm\"]/div/table/tbody/tr[100]/td[4]/div/div\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ab6e188",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WebDriverWait' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-b7a87dad6699>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 2번째 제목 크롤링\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mWebDriverWait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0muntil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpresence_of_element_located\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXPATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"//*[@id='frm']/div/table/tbody/tr[2]/td[4]/div/div\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'WebDriverWait' is not defined"
     ]
    }
   ],
   "source": [
    "# 2번째 제목 크롤링\n",
    "WebDriverWait(driver, 20) \\\n",
    "    .until(EC.presence_of_element_located((By.XPATH, \"//*[@id='frm']/div/table/tbody/tr[2]/td[4]/div/div\"))).text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dee4702",
   "metadata": {},
   "source": [
    "## 1.2. Text Crawling with for loop\n",
    "위에서 찾은 Xpath의 규칙을 바탕으로 for loop 만들자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f37590dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'webdriver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-3b532f18080d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# chrome driver 설정\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdriver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDRIVER_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimplicitly_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://www.melon.com/chart/index.htm\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'webdriver' is not defined"
     ]
    }
   ],
   "source": [
    "# chrome driver 설정\n",
    "driver = webdriver.Chrome(DRIVER_PATH)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "url = \"https://www.melon.com/chart/index.htm\"\n",
    "\n",
    "driver.get(url)\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# 빈 리스트 변수\n",
    "title_list = []\n",
    "\n",
    "# title crawling (TOP 50)\n",
    "for i in range(1, 51):\n",
    "    title = WebDriverWait(driver, 20) \\\n",
    "        .until(EC.presence_of_element_located((By.XPATH, f\"//*[@id='frm']/div/table/tbody/tr[{i}]/td[4]/div/div\")))\n",
    "    title_list.append(title.text)\n",
    "    \n",
    "print(title_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddc69cc",
   "metadata": {},
   "source": [
    "나중에 필요한 변수(제목, 가수, 가사... 등)을 모두 긁어 한번에 데이터프레임으로 저장하여 보관한다!\n",
    "\n",
    "## 1.3. Text Crawling (Click & Back)\n",
    "클릭하고 나오기 -> 동적 크롤링 가능 (가사 크롤링 가능)\n",
    "\n",
    "노래 제목에 링크가 걸려있기 때문에, 해당 링크까지의 XPath를 추가한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88af0c73",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'webdriver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-0d464381fb40>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# chrome driver 설정\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdriver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDRIVER_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimplicitly_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://www.melon.com/chart/index.htm\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'webdriver' is not defined"
     ]
    }
   ],
   "source": [
    " # chrome driver 설정\n",
    "driver = webdriver.Chrome(DRIVER_PATH)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "url = \"https://www.melon.com/chart/index.htm\"\n",
    "\n",
    "driver.get(url)\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# 1번째 click하기\n",
    "click_element = WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"frm\"]/div/table/tbody/tr[1]/td[3]/div/a')))\n",
    "click_element.click()    \n",
    "\n",
    "# back\n",
    "driver.back()\n",
    "\n",
    "\n",
    "# 2번째 click하기\n",
    "click_element = WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"frm\"]/div/table/tbody/tr[2]/td[3]/div/a')))\n",
    "click_element.click()    \n",
    "\n",
    "# back\n",
    "driver.back()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fbdbfc",
   "metadata": {},
   "source": [
    "## 1.4. Text Crawling including contents\n",
    "1.2처럼 for문과 함께 써보자! (첫 페이지 5개의 글에 대해 title, artist, heart(하트 갯수), lyrics(가사)를 크롤링\n",
    "\n",
    "1.3에서 사용한 click & back을 활용하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a86e1245",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'webdriver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-0b8eb48f1d4c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# chrome driver 설정\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdriver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDRIVER_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimplicitly_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://www.melon.com/chart/index.htm\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'webdriver' is not defined"
     ]
    }
   ],
   "source": [
    " # chrome driver 설정\n",
    "driver = webdriver.Chrome(DRIVER_PATH)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "url = \"https://www.melon.com/chart/index.htm\"\n",
    "driver.get(url)\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# 빈 리스트 변수\n",
    "title_list = []\n",
    "artist_list = []\n",
    "heart_list = []\n",
    "lyrics_list = []\n",
    "\n",
    "# crawling (TOP 5)\n",
    "for i in range(1, 6):\n",
    "    # click\n",
    "    click_element = WebDriverWait(driver, 20) \\\n",
    "        .until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"frm\"]/div/table/tbody/tr[{i}]/td[3]/div/a')))\n",
    "    click_element.click()\n",
    "\n",
    "    # title crawling\n",
    "    title = WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"#downloadfrm > div > div > div.entry > div.info > div.song_name\")))\n",
    "    title_list.append(title.text)\n",
    "\n",
    "    # artist crawling\n",
    "    artist = WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"#downloadfrm > div > div > div.entry > div.info > div.artist > a > span:nth-child(1)\")))\n",
    "    artist_list.append(artist.text)\n",
    "    \n",
    "    # heart crawling\n",
    "    heart = WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"#d_like_count\")))\n",
    "    heart_list.append(heart.text)\n",
    "\n",
    "    # lyrics crawling\n",
    "    lyrics = WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"#d_video_summary\")))\n",
    "    lyrics_list.append(lyrics.text)\n",
    "    \n",
    "    # back\n",
    "    driver.back()\n",
    "    \n",
    "print(title_list)\n",
    "print(artist_list)\n",
    "print(heart_list)\n",
    "print(lyrics_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0fe601",
   "metadata": {},
   "source": [
    "## TIP: 보통은 결과값을 데이터프레임 형태로 저장한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a796655",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'title_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-18f62d9094ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 결과 변수\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m raw_result = {'title': title_list,\n\u001b[0m\u001b[0;32m      3\u001b[0m               \u001b[1;34m'artist'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0martist_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m               \u001b[1;34m'heart'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mheart_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m           'lyrics': lyrics_list}\n",
      "\u001b[1;31mNameError\u001b[0m: name 'title_list' is not defined"
     ]
    }
   ],
   "source": [
    "# 결과 변수\n",
    "raw_result = {'title': title_list,\n",
    "              'artist': artist_list,\n",
    "              'heart': heart_list,\n",
    "          'lyrics': lyrics_list}\n",
    "\n",
    "result = pd.DataFrame(raw_result)\n",
    "\n",
    "# # csv 파일로 save\n",
    "# result.to_csv(\"MelonTop5\", mode='w')\n",
    "\n",
    "# driver 종료\n",
    "driver.quit()\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39afb624",
   "metadata": {},
   "source": [
    "**데이터프레임** 형식을 이용하면, 가독성도 좋고, 나중에 데이터 핸들링하기에도 편리하다!\n",
    "\n",
    "# 2. Image Crawling\n",
    "이미지 크롤링하기\n",
    "\n",
    "* 1번째 이미지: /html/body/div/div[3]/div/div/div[4]/form/div/table/tbody/tr[1]/td[4]/div/a/img\n",
    "* 2번째 이미지: /html/body/div/div[3]/div/div/div[4]/form/div/table/tbody/tr[2]/td[4]/div/a/img\n",
    "...\n",
    "\n",
    "* 50번째 이미지: /html/body/div/div[3]/div/div/div[4]/form/div/table/tbody/tr[50]/td[4]/div/a/img\n",
    "\n",
    "\n",
    "**STEP1. URL Crawling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5c26afd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'webdriver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-021bbe86c3e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# chrome driver 설정\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdriver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDRIVER_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimplicitly_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://www.melon.com/chart/index.htm\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'webdriver' is not defined"
     ]
    }
   ],
   "source": [
    " # chrome driver 설정\n",
    "driver = webdriver.Chrome(DRIVER_PATH)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "url = \"https://www.melon.com/chart/index.htm\"\n",
    "\n",
    "driver.get(url)\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# 빈 리스트 변수\n",
    "link_list = []\n",
    "\n",
    "# # img crawling (TOP 50)\n",
    "for i in range(1, 51):\n",
    "    \n",
    "    img = WebDriverWait(driver, 20) \\\n",
    "        .until(EC.presence_of_element_located((By.CSS_SELECTOR, f\"#frm > div > table > tbody > tr:nth-child({i}) > td:nth-child(2) > div > a > img\")))\n",
    "\n",
    "    link_list.append(img.get_attribute('src'))\n",
    "\n",
    "print(link_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859edc13",
   "metadata": {},
   "source": [
    "**STEP2. Download images using URLs**\n",
    "\n",
    "자신의 디렉토리에 img 폴더 생성하고 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6400cfe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'link_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-7c6c9ecf0783>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlink_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mcount\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'./img/img'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'link_list' is not defined"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "count = 0\n",
    "for link in link_list:\n",
    "    count += 1\n",
    "    urllib.request.urlretrieve(link, './img/img' + str(count) + '.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
